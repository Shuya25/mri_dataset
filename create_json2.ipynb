{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# == Base ==\n",
    "DATA_DIR = pathlib.Path(\"/data2\") / \"radiology_datas\"\n",
    "# == Dataset ==\n",
    "ADNI1 = DATA_DIR / \"adni1\" / \"stripped_cloud\"\n",
    "ADNI2 = DATA_DIR / \"adni2\" / \"stripped_cloud\"\n",
    "ADNI2_2 = DATA_DIR / \"adni2_2\" / \"stripped_cloud\"\n",
    "PPMI = DATA_DIR / \"PPMI\" / \"stripped_cloud\"\n",
    "FourRTNI = DATA_DIR / \"4RTNI\" /\"stripped_skull\"\n",
    "\n",
    "BLACKLIST_DIR = DATA_DIR / \"black_lists\"\n",
    "NOT_CSV_DIR = DATA_DIR / \"not_csv\"\n",
    "\n",
    "DATA_CSV = {\n",
    "    \"ADNI\": DATA_DIR / \"csv\" / \"ADNIMERGE.csv\",\n",
    "    \"PPMI\": DATA_DIR / \"csv\" / \"PPMI.csv\",\n",
    "    \"4RTNI\": DATA_DIR / \"csv\" / \"4RTNI_DATA.csv\",\n",
    "    \"ADNI_s\": DATA_DIR / \"csv\" / \"ADNI_summary_sheet.csv\",\n",
    "}\n",
    "\n",
    "DATA_DIRS_DICT = {\n",
    "    \"ADNI1\": ADNI1,\n",
    "    \"ADNI2\": ADNI2,\n",
    "    \"ADNI2-2\": ADNI2_2,\n",
    "    \"PPMI\": PPMI,\n",
    "    \"4RTNI\": FourRTNI\n",
    "}\n",
    "\n",
    "PTID = {\"ADNI\": \"PTID\", \"PPMI\": \"Subject\", \"4RTNI\": \"SUBID\", \"ADNI_s\": \"subjectIdentifier\"}\n",
    "PTCLASS = {\"ADNI\": \"DX_bl\", \"PPMI\": \"Group\", \"4RTNI\": \"DX\", \"ADNI_s\": \"subjectInfo: DX Group\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uid(path):\n",
    "    uid = path.name\n",
    "    for key, value in DATA_DIRS_DICT.items():\n",
    "        if str(value) in str(path):\n",
    "\n",
    "            if key == \"ADNI2\":\n",
    "                uid = path.name.split(\"_\")[-2]\n",
    "                uid = int(uid[1:])\n",
    "\n",
    "            elif key == \"ADNI2-2\":\n",
    "                uid = path.name.split(\"_\")[-4]\n",
    "                uid = int(uid[1:])\n",
    "\n",
    "            elif key == \"PPMI\":\n",
    "                uid = path.name.split(\"_\")[-4]\n",
    "                uid = int(uid)\n",
    "\n",
    "            elif key == \"4RTNI\":\n",
    "                uid = path.name.split(\"_\")[-4]\n",
    "                uid = int(uid)\n",
    "\n",
    "            return uid\n",
    "\n",
    "def get_blacklist(dir):\n",
    "    key = \"**/uids.txt\"\n",
    "    excluded_uid_paths = dir.glob(key)\n",
    "    excluded_uids = []\n",
    "    for path in excluded_uid_paths:\n",
    "        with open(path, \"r\") as rf:\n",
    "            [excluded_uids.append(int(uid.rstrip(\"\\n\"))) for uid in rf]\n",
    "    return excluded_uids\n",
    "\n",
    "def get_not_csv_list(dir):\n",
    "    key = \"**/pids.txt\"\n",
    "    excluded_uid_paths = dir.glob(key)\n",
    "    excluded_uids = []\n",
    "    for path in excluded_uid_paths:\n",
    "        with open(path, \"r\") as rf:\n",
    "            [excluded_uids.append(uid.rstrip(\"\\n\")) for uid in rf]\n",
    "    return excluded_uids\n",
    "    \n",
    "\n",
    "black_list = get_blacklist(BLACKLIST_DIR)\n",
    "not_csv_list = get_not_csv_list(NOT_CSV_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "\n",
    "key = \"**/pids.txt\"\n",
    "excluded_uid_paths = NOT_CSV_DIR.glob(key)\n",
    "for path in excluded_uid_paths:\n",
    "    with open(path, \"r\") as rf:\n",
    "        excluded_uids=[]\n",
    "        ptclass = str(path).split(\"/\")[-2]\n",
    "        dataset = str(path).split(\"/\")[-3]\n",
    "\n",
    "        [excluded_uids.append(uid.rstrip(\"\\n\")) for uid in rf]\n",
    "        \n",
    "        add_list = defaultdict(lambda :defaultdict(list))\n",
    "        for ptid in excluded_uids:\n",
    "            \n",
    "            path = pathlib.Path(DATA_DIRS_DICT[dataset] / ptclass / ptid)\n",
    "            file = [p for p in path.glob('**/*') if re.search('/*\\.(pkl|npy)', str(p))]\n",
    "            for f in file:\n",
    "                f_temp = str(f).split(\"/\")[-1]\n",
    "                uid = get_uid(f)\n",
    "                #print(f)\n",
    "\n",
    "                if f_temp.startswith(\"fullsize\"):\n",
    "                    add_list[ptid][uid].append((\"full\", str(f)))\n",
    "\n",
    "                if f_temp.startswith(\"half\"):\n",
    "                    add_list[ptid][uid].append((\"half\", str(f)))\n",
    "        \n",
    "        for ptid, add_uid in add_list.items():\n",
    "            images = []\n",
    "            for uid, ls in add_uid.items():\n",
    "                if uid not in black_list:\n",
    "                    blacklisted = False\n",
    "                else:\n",
    "                    blacklisted = True\n",
    "\n",
    "                for l in ls:\n",
    "                    if l[0] == \"full\":\n",
    "                        full = l[1]\n",
    "                    elif l[0] == \"half\":\n",
    "                        half = l[1]\n",
    "                images.append(\n",
    "                    {\n",
    "                        \"uid\": uid,\n",
    "                        \"blacklisted\": blacklisted,\n",
    "                        \"fullsize_img_path\": full,\n",
    "                        \"halfsize_img_path\": half\n",
    "                    }\n",
    "                )  \n",
    "            content = {\n",
    "                \"id\": ptid,\n",
    "                \"class\": ptclass,\n",
    "                \"images\": images,\n",
    "                \"dataset\": dataset,\n",
    "                \"not_csv\": True\n",
    "            }\n",
    "            contents.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12729/12729 [00:01<00:00, 10783.19it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2279.36it/s]\n",
      "  0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADNI1 /data2/radiology_datas/adni1/stripped_cloud nan 4_S_5010\n",
      "ADNI2 /data2/radiology_datas/adni2/stripped_cloud nan 4_S_5010\n",
      "ADNI2-2 /data2/radiology_datas/adni2_2/stripped_cloud nan 4_S_5010\n",
      "PPMI /data2/radiology_datas/PPMI/stripped_cloud nan 4_S_5010\n",
      "4RTNI /data2/radiology_datas/4RTNI/stripped_skull nan 4_S_5010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:00<00:00, 1191.95it/s]\n",
      "100%|██████████| 3600/3600 [00:00<00:00, 9918.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1772\n"
     ]
    }
   ],
   "source": [
    "#contents = []\n",
    "\n",
    "ptid_list = set([])\n",
    "for csv_label, csv in DATA_CSV.items():\n",
    "    if csv_label == \"ADNI_s\":\n",
    "        #df = pd.read_excel(csv, engine='openpyxl')\n",
    "        df = pd.read_csv(csv)\n",
    "    else:\n",
    "        df = pd.read_csv(csv)\n",
    "    if csv_label == \"4RTNI\":\n",
    "        df[\"DX\"] = df[\"DX\"].map({\"CBS\": \"CBD\", \"PSP\": \"PSP\", \"Oth\": \"Oth\"}, na_action=None)\n",
    "\n",
    "    for data in tqdm(df.to_dict(orient=\"records\")):\n",
    "        ptclass = data[PTCLASS[csv_label]]\n",
    "        ptid = str(data[PTID[csv_label]])\n",
    "        \n",
    "        if (ptid, ptclass) in ptid_list:\n",
    "            continue\n",
    "        else:\n",
    "            ptid_list.add((ptid, ptclass))\n",
    "            \n",
    "        files = []\n",
    "        for label, dir in DATA_DIRS_DICT.items():\n",
    "            try:\n",
    "                path = pathlib.Path(dir / ptclass / ptid)\n",
    "            except TypeError:\n",
    "                path = pathlib.Path(dir / \"Nan\" / ptid)\n",
    "                print(label, dir, ptclass, ptid, flush=True)\n",
    "                \n",
    "            file = [p for p in path.glob('**/*') if re.search('/*\\.(pkl|npy)', str(p))]\n",
    "            # files = list(path.glob(\"*.[p,n][k,p][l,y]\"))\n",
    "            if len(file) != 0:\n",
    "                files += file\n",
    "                dataset = label\n",
    "\n",
    "        add_list = defaultdict(lambda :defaultdict(list))\n",
    "        for f in files:\n",
    "            f_temp = str(f).split(\"/\")[-1]\n",
    "            uid = get_uid(f)\n",
    "            \n",
    "            if f_temp.startswith(\"fullsize\"):\n",
    "                add_list[ptid][uid].append((\"full\", str(f)))\n",
    "                \n",
    "            if f_temp.startswith(\"half\"):\n",
    "                add_list[ptid][uid].append((\"half\", str(f)))\n",
    "            \n",
    "                \n",
    "        for ptid, add_uid in add_list.items():\n",
    "            images = []\n",
    "            \n",
    "            if ptid not in not_csv_list:\n",
    "                not_csv = False\n",
    "            else:\n",
    "                not_csv = True\n",
    "            \n",
    "            for uid, ls in add_uid.items():\n",
    "                if uid not in black_list:\n",
    "                    blacklisted = False\n",
    "                else:\n",
    "                    blacklisted = True\n",
    "                    \n",
    "                for l in ls:\n",
    "                    if l[0] == \"full\":\n",
    "                        full = l[1]\n",
    "                    elif l[0] == \"half\":\n",
    "                        half = l[1]\n",
    "            \n",
    "                images.append(\n",
    "                    {\n",
    "                        \"uid\": uid,\n",
    "                        \"blacklisted\": blacklisted,\n",
    "                        \"fullsize_img_path\": full,\n",
    "                        \"halfsize_img_path\": half\n",
    "                    }\n",
    "                )     \n",
    "            content = {\n",
    "                \"id\": ptid,\n",
    "                \"class\": ptclass,\n",
    "                \"images\": images,\n",
    "                \"dataset\": dataset,\n",
    "                \"not_csv\": False\n",
    "            }\n",
    "            contents.append(content)\n",
    "\n",
    "  # break\n",
    "\n",
    "with open('./all_subjects.json', 'w') as f:\n",
    "    json.dump(contents, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(len(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "source": [
    "from load_images import load_images\n",
    "D = {'ADNI2','ADNI2-2'}\n",
    "C = {'AD'}\n",
    "U = True\n",
    "B = True\n",
    "N = True\n",
    "\n",
    "images = load_images(datasets=D, classes=C, size=\"half\", unique=U, blacklist=B, add_csv=N, dryrun=True)\n",
    "print(len(images))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "304c78370837cbdd5ebe267a00a40f4c1d9b05b3e15542e195d519cc05296b47"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('3.8.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
